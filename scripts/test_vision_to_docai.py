#!/usr/bin/env python3
"""
Vision â†’ DocAI Pipeline Diagnostics Script

This script validates the complete document processing pipeline:
1. PDF â†’ Vision OCR (raw + normalized)
2. Vision output â†’ DocAI processing (raw + parsed)
3. Comprehensive analysis and comparison
4. Diagnostic reporting with prioritized fixes

Follows the exact specifications for automated diagnostics.
"""

import asyncio
import json
import logging
import os
import re
import shutil
import sys
import time
import traceback
from datetime import datetime
from difflib import unified_diff
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
import uuid

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# Third-party imports
import httpx
import importlib.util

def get_gcs_test_bucket() -> str:
    """Get GCS test bucket from environment, with fallback."""
    bucket = os.getenv('GCS_TEST_BUCKET', 'gs://test-bucket/')
    return bucket.rstrip('/') + '/'
from dotenv import load_dotenv
from fastapi.testclient import TestClient

# Load environment
load_dotenv()

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class VisionDocAIDiagnostics:
    """
    Comprehensive diagnostics for Vision â†’ DocAI pipeline.
    
    Tests the complete flow from PDF through Vision OCR to DocAI processing,
    generating detailed artifacts and analysis reports.
    """
    
    def __init__(self):
        """Initialize diagnostics with configuration."""
        
        self.project_root = project_root
        self.artifacts_dir = self.project_root / "artifacts" / "vision_to_docai"
        self.artifacts_dir.mkdir(parents=True, exist_ok=True)
        
        # Configuration
        self.config = {
            "google_project_id": os.getenv("GOOGLE_CLOUD_PROJECT_ID"),
            "google_credentials": os.getenv("GOOGLE_APPLICATION_CREDENTIALS"),
            "docai_location": os.getenv("DOCAI_LOCATION", "us"),
            "docai_processor_id": os.getenv("DOCAI_PROCESSOR_ID"),
            "confidence_threshold": float(os.getenv("DOCAI_CONFIDENCE_THRESHOLD", "0.7"))
        }
        
        # Test data
        self.test_pdf_path = self.project_root / "data" / "test-files" / "testing-ocr-pdf-1.pdf"
        
        # Results storage
        self.results = {
            "vision_ocr": {},
            "docai_processing": {},
            "comparison": {},
            "diagnostics": {},
            "timing": {},
            "errors": []
        }
        
        # Initialize FastAPI client
        try:
            from main import app
            self.test_client = TestClient(app)
            logger.info("FastAPI test client initialized")
        except Exception as e:
            logger.error(f"Failed to initialize FastAPI client: {e}")
            self.test_client = None
        
        logger.info(f"Diagnostics initialized. Artifacts dir: {self.artifacts_dir}")
    
    def _process_existing_vision_data(self, ocr_file_path: Path, start_time: float) -> Dict[str, Any]:
        """Process existing Vision OCR data."""
        
        with open(ocr_file_path, encoding='utf-8') as f:
            vision_data = json.load(f)
        
        # Save as vision_raw.json
        with open(self.artifacts_dir / "vision_raw.json", 'w', encoding='utf-8') as f:
            json.dump(vision_data, f, indent=2)
        
        # Create normalized version
        normalized = self._normalize_vision_output(vision_data)
        with open(self.artifacts_dir / "vision_normalized.json", 'w', encoding='utf-8') as f:
            json.dump(normalized, f, indent=2)
        
        processing_time = time.time() - start_time
        self.results["timing"]["vision_ocr"] = processing_time
        
        self.results["vision_ocr"] = {
            "success": True,
            "source": "existing_data",
            "processing_time": processing_time,
            "pages_processed": len(vision_data.get("ocr_result", {}).get("pages", [])),
            "full_text_length": len(vision_data.get("ocr_result", {}).get("full_text", "")),
            "language_detected": vision_data.get("language_detection", {}).get("primary", "unknown")
        }
        
        logger.info(f"âœ… Vision OCR (existing data): {self.results['vision_ocr']['pages_processed']} pages")
        return self.results["vision_ocr"]

    def load_vision_ocr_module(self):
        """Load the Vision OCR module with proper import handling."""
        try:
            # Try direct import first
            from services.preprocessing.ocr_processing import GoogleVisionOCR
            return GoogleVisionOCR
        except ImportError:
            # Handle hyphenated filename
            ocr_module_path = self.project_root / "services" / "preprocessing" / "OCR-processing.py"
            if ocr_module_path.exists():
                spec = importlib.util.spec_from_file_location("ocr_processing", ocr_module_path)
                ocr_module = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(ocr_module)
                return ocr_module.GoogleVisionOCR
            else:
                raise ImportError("Could not find Vision OCR module")
    
    def run_vision_ocr_processing(self) -> Dict[str, Any]:
        """
        Process test PDF through Vision OCR and save raw/normalized outputs.
        
        Returns:
            Vision processing results
        """
        logger.info("=" * 60)
        logger.info("STEP 1: VISION OCR PROCESSING")
        logger.info("=" * 60)
        
        start_time = time.time()
        
        try:
            # Check if test PDF exists
            if not self.test_pdf_path.exists():
                logger.warning(f"Test PDF not found: {self.test_pdf_path}")
                # Use existing processed data if available
                existing_ocr = self.project_root / "data" / "testing-ocr-pdf-1-1e08491e-28e026de" / "testing-ocr-pdf-1-1e08491e-28e026de.json"
                if existing_ocr.exists():
                    logger.info(f"Using existing OCR data: {existing_ocr}")
                    with open(existing_ocr) as f:
                        vision_data = json.load(f)
                    
                    # Save as vision_raw.json
                    with open(self.artifacts_dir / "vision_raw.json", 'w') as f:
                        json.dump(vision_data, f, indent=2)
                    
                    # Create normalized version
                    normalized = self._normalize_vision_output(vision_data)
                    with open(self.artifacts_dir / "vision_normalized.json", 'w') as f:
                        json.dump(normalized, f, indent=2)
                    
                    processing_time = time.time() - start_time
                    self.results["timing"]["vision_ocr"] = processing_time
                    
                    self.results["vision_ocr"] = {
                        "success": True,
                        "source": "existing_data",
                        "processing_time": processing_time,
                        "pages_processed": len(vision_data.get("ocr_result", {}).get("pages", [])),
                        "full_text_length": len(vision_data.get("ocr_result", {}).get("full_text", "")),
                        "language_detected": vision_data.get("language_detection", {}).get("primary", "unknown")
                    }
                    
                    logger.info(f"âœ… Vision OCR (existing data): {self.results['vision_ocr']['pages_processed']} pages")
                    return self.results["vision_ocr"]
                else:
                    raise FileNotFoundError("No test PDF or existing OCR data found")
            
            # Load Vision OCR module
            GoogleVisionOCR = self.load_vision_ocr_module()
            
            # Initialize Vision OCR client
            ocr_client = GoogleVisionOCR.from_env()
            
            # First, convert PDF to images using util services
            from services.util_services import PDFToImageConverter
            
            converter = PDFToImageConverter(
                data_root=str(self.project_root / "data"),
                image_format="PNG",
                dpi=300
            )
            
            # Convert PDF to images
            logger.info(f"Converting PDF to images: {self.test_pdf_path}")
            uid, image_paths, metadata = converter.convert_pdf_to_images(
                str(self.test_pdf_path),
                output_folder=str(self.artifacts_dir / "vision_images")
            )
            
            logger.info(f"Generated {len(image_paths)} images with UID: {uid}")
            
            # Process each image with Vision OCR
            ocr_results = []
            full_text_parts = []
            
            for i, image_path in enumerate(image_paths):
                logger.info(f"Processing image {i+1}/{len(image_paths)}: {Path(image_path).name}")
                
                ocr_result = ocr_client.extract_text(
                    image_path=image_path,
                    page_number=i+1
                )
                
                if ocr_result.get("success", False):
                    page_data = ocr_result.get("page_data", {})
                    page_text = " ".join([
                        block.get("text", "") 
                        for block in page_data.get("text_blocks", [])
                    ])
                    full_text_parts.append(page_text)
                    ocr_results.append(ocr_result)
                    
                    logger.info(f"   âœ… Page {i+1}: {len(page_data.get('text_blocks', []))} blocks")
                else:
                    logger.error(f"   âŒ Page {i+1} failed: {ocr_result.get('error', 'Unknown error')}")
                    self.results["errors"].append(f"Vision OCR failed for page {i+1}")
            
            # Compile complete Vision output
            vision_raw = {
                "document_id": uid,
                "original_filename": self.test_pdf_path.name,
                "processing_timestamp": datetime.now().isoformat(),
                "pages_processed": len(image_paths),
                "pages_successful": len(ocr_results),
                "full_text": "\n".join(full_text_parts),
                "ocr_results": ocr_results,
                "metadata": metadata
            }
            
            # Save raw Vision output
            with open(self.artifacts_dir / "vision_raw.json", 'w') as f:
                json.dump(vision_raw, f, indent=2)
            
            # Create normalized version
            normalized = self._normalize_vision_output(vision_raw)
            with open(self.artifacts_dir / "vision_normalized.json", 'w') as f:
                json.dump(normalized, f, indent=2)
            
            processing_time = time.time() - start_time
            self.results["timing"]["vision_ocr"] = processing_time
            
            self.results["vision_ocr"] = {
                "success": len(ocr_results) > 0,
                "source": "live_processing",
                "processing_time": processing_time,
                "pages_processed": len(image_paths),
                "pages_successful": len(ocr_results),
                "full_text_length": len(vision_raw["full_text"]),
                "document_uid": uid
            }
            
            logger.info(f"âœ… Vision OCR completed: {len(ocr_results)}/{len(image_paths)} pages successful")
            return self.results["vision_ocr"]
            
        except Exception as e:
            logger.error(f"âŒ Vision OCR processing failed: {e}")
            traceback.print_exc()
            self.results["errors"].append(f"Vision OCR error: {str(e)}")
            return {"success": False, "error": str(e)}
    
    def _normalize_vision_output(self, vision_raw: Dict[str, Any]) -> Dict[str, Any]:
        """Normalize Vision OCR output to standard format."""
        
        try:
            # Extract full text
            if "full_text" in vision_raw:
                full_text = vision_raw["full_text"]
            elif "ocr_result" in vision_raw:
                full_text = vision_raw["ocr_result"].get("full_text", "")
            else:
                full_text = ""
            
            # Extract pages data
            pages = []
            if "ocr_results" in vision_raw:
                for result in vision_raw["ocr_results"]:
                    page_data = result.get("page_data", {})
                    pages.append({
                        "page_number": page_data.get("page", 1),
                        "confidence": page_data.get("page_confidence", 0.0),
                        "text_blocks_count": len(page_data.get("text_blocks", [])),
                        "text_blocks": page_data.get("text_blocks", [])
                    })
            elif "ocr_result" in vision_raw and "pages" in vision_raw["ocr_result"]:
                pages = vision_raw["ocr_result"]["pages"]
            
            normalized = {
                "document_id": vision_raw.get("document_id", "unknown"),
                "full_text": full_text,
                "full_text_length": len(full_text),
                "pages": pages,
                "page_count": len(pages),
                "language_detection": vision_raw.get("language_detection", {}),
                "processing_metadata": {
                    "timestamp": vision_raw.get("processing_timestamp", datetime.now().isoformat()),
                    "source": "vision_ocr",
                    "total_blocks": sum(page.get("text_blocks_count", 0) for page in pages)
                }
            }
            
            return normalized
            
        except Exception as e:
            logger.error(f"Vision normalization error: {e}")
            return {"error": str(e)}
    
    def run_docai_processing(self) -> Dict[str, Any]:
        """
        Process through DocAI and save raw response and parsed output.
        
        Returns:
            DocAI processing results
        """
        logger.info("=" * 60)
        logger.info("STEP 2: DOCAI PROCESSING")
        logger.info("=" * 60)
        
        start_time = time.time()
        
        try:
            if not self.test_client:
                raise Exception("FastAPI test client not available")
            
            # Method 1: Try with existing test PDF
            logger.info("Testing DocAI with test PDF...")
            
            # Check if we can read the PDF file
            if self.test_pdf_path.exists():
                # Test both staging and direct GCS modes
                
                # Test 1: Local file staging (new functionality)
                logger.info("Testing local file staging to GCS...")
                parse_request = {
                    "gcs_uri": str(self.test_pdf_path),  # Use local path to test staging
                    "confidence_threshold": self.config["confidence_threshold"],
                    "enable_native_pdf_parsing": True
                }
                
                logger.info("Sending DocAI parse request with local file path...")
                response = self.test_client.post("/api/docai/parse", json=parse_request)
                
                logger.info(f"DocAI response status: {response.status_code}")
                
                if response.status_code == 200:
                    result_data = response.json()
                    
                    # Save raw DocAI response
                    with open(self.artifacts_dir / "docai_raw.json", 'w') as f:
                        json.dump(result_data, f, indent=2)
                    
                    # Extract parsed document if available
                    parsed_doc = result_data.get("parsed_document", {})
                    with open(self.artifacts_dir / "parsed_output.json", 'w') as f:
                        json.dump(parsed_doc, f, indent=2)
                    
                    processing_time = time.time() - start_time
                    self.results["timing"]["docai_processing"] = processing_time
                    
                    # Analyze DocAI response
                    analysis = self._analyze_docai_response(result_data)
                    
                    self.results["docai_processing"] = {
                        "success": result_data.get("success", False),
                        "processing_time": processing_time,
                        "response_size": len(json.dumps(result_data)),
                        "analysis": analysis
                    }
                    
                    if result_data.get("success", False):
                        logger.info(f"âœ… DocAI processing successful")
                        logger.info(f"   Entities: {analysis.get('entity_count', 0)}")
                        logger.info(f"   Clauses: {analysis.get('clause_count', 0)}")
                        logger.info(f"   Text length: {analysis.get('text_length', 0)}")
                    else:
                        error_msg = result_data.get("error_message", "Unknown error")
                        logger.warning(f"âš ï¸ DocAI returned error: {error_msg}")
                        # This might be expected due to GCS access in test environment
                    
                    return self.results["docai_processing"]
                    
                else:
                    logger.error(f"DocAI endpoint failed: {response.status_code}")
                    logger.error(f"Response: {response.text}")
                    
                    # Try to save error response for analysis
                    try:
                        error_data = response.json()
                        with open(self.artifacts_dir / "docai_error.json", 'w') as f:
                            json.dump(error_data, f, indent=2)
                    except:
                        with open(self.artifacts_dir / "docai_error.txt", 'w') as f:
                            f.write(response.text)
                    
                    self.results["errors"].append(f"DocAI endpoint error: {response.status_code}")
                    return {"success": False, "error": f"HTTP {response.status_code}"}
            
            else:
                # Use existing processed DocAI data
                logger.info("Test PDF not found, checking for existing DocAI data...")
                existing_docai = self.project_root / "data" / "processed" / "docai_raw_20250918_124117.json"
                
                if existing_docai.exists():
                    logger.info(f"Using existing DocAI data: {existing_docai}")
                    
                    with open(existing_docai) as f:
                        docai_data = json.load(f)
                    
                    # Save as artifacts
                    with open(self.artifacts_dir / "docai_raw.json", 'w') as f:
                        json.dump(docai_data, f, indent=2)
                    
                    # Create parsed output (docai_raw is already processed)
                    with open(self.artifacts_dir / "parsed_output.json", 'w') as f:
                        json.dump(docai_data, f, indent=2)
                    
                    processing_time = time.time() - start_time
                    self.results["timing"]["docai_processing"] = processing_time
                    
                    analysis = self._analyze_docai_response(docai_data)
                    
                    self.results["docai_processing"] = {
                        "success": True,
                        "source": "existing_data",
                        "processing_time": processing_time,
                        "analysis": analysis
                    }
                    
                    logger.info(f"âœ… DocAI data loaded from existing file")
                    logger.info(f"   Text length: {analysis.get('text_length', 0)}")
                    logger.info(f"   Page count: {analysis.get('page_count', 0)}")
                    
                    return self.results["docai_processing"]
                    
                else:
                    raise FileNotFoundError("No test PDF or existing DocAI data found")
        
        except Exception as e:
            logger.error(f"âŒ DocAI processing failed: {e}")
            traceback.print_exc()
            self.results["errors"].append(f"DocAI processing error: {str(e)}")
            return {"success": False, "error": str(e)}
    
    def _analyze_docai_response(self, docai_data: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze DocAI response and extract key metrics."""
        
        analysis = {
            "text_length": 0,
            "page_count": 0,
            "entity_count": 0,
            "clause_count": 0,
            "key_value_pairs": [],
            "has_offsets": False,
            "has_entities": False,
            "has_clauses": False,
            "has_cross_references": False
        }
        
        try:
            # Extract text information
            if "text" in docai_data:
                analysis["text_length"] = len(docai_data["text"])
                analysis["page_count"] = docai_data.get("page_count", 0)
                analysis["entity_count"] = docai_data.get("entity_count", 0)
            
            # Check for entities
            if "entities" in docai_data:
                analysis["entity_count"] = len(docai_data["entities"])
                analysis["has_entities"] = True
            
            # Check for clauses
            if "clauses" in docai_data:
                analysis["clause_count"] = len(docai_data["clauses"])
                analysis["has_clauses"] = True
            
            # Check for key-value pairs
            if "key_value_pairs" in docai_data:
                analysis["key_value_pairs"] = docai_data["key_value_pairs"]
            
            # Check for cross-references
            if "cross_references" in docai_data:
                analysis["has_cross_references"] = len(docai_data["cross_references"]) > 0
            
            # Check for offset information
            if "entities" in docai_data and docai_data["entities"]:
                first_entity = docai_data["entities"][0]
                if "start_offset" in first_entity and "end_offset" in first_entity:
                    analysis["has_offsets"] = True
            
        except Exception as e:
            logger.error(f"DocAI analysis error: {e}")
            analysis["error"] = str(e)
        
        return analysis
    
    def run_automated_checks(self) -> Dict[str, Any]:
        """
        Run automated checks comparing Vision and DocAI outputs.
        
        Returns:
            Automated check results
        """
        logger.info("=" * 60)
        logger.info("STEP 3: AUTOMATED CHECKS")
        logger.info("=" * 60)
        
        try:
            # Load Vision and DocAI data
            vision_raw_path = self.artifacts_dir / "vision_raw.json"
            docai_raw_path = self.artifacts_dir / "docai_raw.json"
            
            if not vision_raw_path.exists() or not docai_raw_path.exists():
                raise FileNotFoundError("Vision or DocAI data not found for comparison")
            
            with open(vision_raw_path) as f:
                vision_data = json.load(f)
            
            with open(docai_raw_path) as f:
                docai_data = json.load(f)
            
            # Check 1: Text comparison
            logger.info("1ï¸âƒ£ Comparing Vision vs DocAI text...")
            text_comparison = self._compare_texts(vision_data, docai_data)
            
            # Check 2: Offset validation
            logger.info("2ï¸âƒ£ Validating offsets...")
            offset_validation = self._validate_offsets(docai_data)
            
            # Check 3: Statistics computation
            logger.info("3ï¸âƒ£ Computing statistics...")
            vision_stats = self._compute_vision_stats(vision_data)
            docai_stats = self._compute_docai_stats(docai_data)
            
            # Check 4: Fallback regex extraction
            logger.info("4ï¸âƒ£ Running fallback extractions...")
            fallback_kv = self._extract_fallback_kvs(vision_data)
            
            # Save all results
            self._save_check_results(text_comparison, offset_validation, vision_stats, docai_stats, fallback_kv)
            
            # Compile diagnostics
            diagnostics = self._compile_diagnostics(text_comparison, offset_validation, vision_stats, docai_stats)
            
            with open(self.artifacts_dir / "diagnostics.json", 'w') as f:
                json.dump(diagnostics, f, indent=2)
            
            self.results["comparison"] = {
                "text_match": text_comparison["exact_match"],
                "text_similarity": text_comparison["similarity_score"],
                "offsets_valid": offset_validation["all_valid"],
                "offset_failures": len(offset_validation["failures"]),
                "diagnostics": diagnostics
            }
            
            logger.info("âœ… Automated checks completed")
            return self.results["comparison"]
            
        except Exception as e:
            logger.error(f"âŒ Automated checks failed: {e}")
            traceback.print_exc()
            self.results["errors"].append(f"Automated checks error: {str(e)}")
            return {"success": False, "error": str(e)}
    
    def _compare_texts(self, vision_data: Dict, docai_data: Dict) -> Dict[str, Any]:
        """Compare Vision OCR text with DocAI text."""
        
        # Extract texts
        vision_text = ""
        if "full_text" in vision_data:
            vision_text = vision_data["full_text"]
        elif "ocr_result" in vision_data:
            vision_text = vision_data["ocr_result"].get("full_text", "")
        
        docai_text = docai_data.get("text", "")
        
        # Normalize for comparison (remove extra whitespace)
        vision_clean = " ".join(vision_text.split())
        docai_clean = " ".join(docai_text.split())
        
        # Compare
        exact_match = vision_clean == docai_clean
        
        # Calculate similarity
        if vision_clean and docai_clean:
            # Simple character-level similarity
            max_len = max(len(vision_clean), len(docai_clean))
            if max_len > 0:
                import difflib
                similarity = difflib.SequenceMatcher(None, vision_clean, docai_clean).ratio()
            else:
                similarity = 0.0
        else:
            similarity = 0.0
        
        # Generate diff
        diff_lines = list(unified_diff(
            vision_text.splitlines()[:20],  # First 20 lines
            docai_text.splitlines()[:20],
            fromfile="vision_ocr.txt",
            tofile="docai.txt",
            lineterm=""
        ))
        
        # Save diff
        with open(self.artifacts_dir / "text_diff.txt", 'w') as f:
            f.write(f"Vision Text Length: {len(vision_text)}\n")
            f.write(f"DocAI Text Length: {len(docai_text)}\n")
            f.write(f"Exact Match: {exact_match}\n")
            f.write(f"Similarity Score: {similarity:.4f}\n")
            f.write("\n" + "=" * 50 + "\n")
            f.write("TEXT DIFF (first 20 lines):\n")
            f.write("\n".join(diff_lines))
            
            # Add first 200 chars comparison
            f.write(f"\n\nFIRST 200 CHARACTERS:\n")
            f.write(f"Vision: {repr(vision_text[:200])}\n")
            f.write(f"DocAI:  {repr(docai_text[:200])}\n")
        
        return {
            "exact_match": exact_match,
            "similarity_score": similarity,
            "vision_length": len(vision_text),
            "docai_length": len(docai_text),
            "first_200_chars": {
                "vision": vision_text[:200],
                "docai": docai_text[:200]
            }
        }
    
    def _validate_offsets(self, docai_data: Dict[str, Any]) -> Dict[str, Any]:
        """Validate that entity offsets map correctly to full text."""
        
        full_text = docai_data.get("text", "")
        entities = docai_data.get("entities", [])
        
        validation_result = {
            "all_valid": True,
            "total_entities": len(entities),
            "valid_offsets": 0,
            "invalid_offsets": 0,
            "failures": []
        }
        
        for i, entity in enumerate(entities):
            entity_id = entity.get("id", f"entity_{i}")
            start_offset = entity.get("start_offset")
            end_offset = entity.get("end_offset")
            expected_text = entity.get("text", entity.get("mention_text", ""))
            
            if start_offset is not None and end_offset is not None:
                if start_offset >= 0 and end_offset <= len(full_text) and start_offset < end_offset:
                    actual_text = full_text[start_offset:end_offset]
                    
                    if actual_text == expected_text:
                        validation_result["valid_offsets"] += 1
                    else:
                        validation_result["invalid_offsets"] += 1
                        validation_result["all_valid"] = False
                        validation_result["failures"].append({
                            "entity_id": entity_id,
                            "start_offset": start_offset,
                            "end_offset": end_offset,
                            "expected_text": expected_text,
                            "actual_text": actual_text,
                            "issue": "text_mismatch"
                        })
                else:
                    validation_result["invalid_offsets"] += 1
                    validation_result["all_valid"] = False
                    validation_result["failures"].append({
                        "entity_id": entity_id,
                        "start_offset": start_offset,
                        "end_offset": end_offset,
                        "issue": "invalid_range",
                        "full_text_length": len(full_text)
                    })
            else:
                validation_result["failures"].append({
                    "entity_id": entity_id,
                    "issue": "missing_offsets"
                })
        
        # Save mismatch report
        with open(self.artifacts_dir / "mismatch_report.json", 'w') as f:
            json.dump(validation_result, f, indent=2)
        
        return validation_result
    
    def _compute_vision_stats(self, vision_data: Dict[str, Any]) -> Dict[str, Any]:
        """Compute Vision OCR statistics."""
        
        stats = {
            "full_text_len": 0,
            "page_count": 0,
            "total_blocks": 0,
            "total_lines": 0,
            "total_words": 0,
            "avg_confidence": 0.0,
            "language_detection": {}
        }
        
        try:
            # Get full text length
            if "full_text" in vision_data:
                stats["full_text_len"] = len(vision_data["full_text"])
            elif "ocr_result" in vision_data:
                stats["full_text_len"] = len(vision_data["ocr_result"].get("full_text", ""))
            
            # Get page information
            pages = []
            if "ocr_result" in vision_data and "pages" in vision_data["ocr_result"]:
                pages = vision_data["ocr_result"]["pages"]
            elif "ocr_results" in vision_data:
                pages = [result.get("page_data", {}) for result in vision_data["ocr_results"]]
            
            stats["page_count"] = len(pages)
            
            # Count blocks, lines, words and calculate confidence
            total_confidence = 0.0
            confidence_count = 0
            
            for page in pages:
                text_blocks = page.get("text_blocks", [])
                stats["total_blocks"] += len(text_blocks)
                
                for block in text_blocks:
                    lines = block.get("lines", [])
                    stats["total_lines"] += len(lines)
                    
                    block_confidence = block.get("confidence", 0.0)
                    if block_confidence > 0:
                        total_confidence += block_confidence
                        confidence_count += 1
                    
                    for line in lines:
                        words = line.get("words", [])
                        stats["total_words"] += len(words)
            
            if confidence_count > 0:
                stats["avg_confidence"] = total_confidence / confidence_count
            
            # Language detection
            stats["language_detection"] = vision_data.get("language_detection", {})
            
        except Exception as e:
            logger.error(f"Vision stats computation error: {e}")
            stats["error"] = str(e)
        
        # Save vision summary
        with open(self.artifacts_dir / "vision_summary.json", 'w') as f:
            json.dump(stats, f, indent=2)
        
        return stats
    
    def _compute_docai_stats(self, docai_data: Dict[str, Any]) -> Dict[str, Any]:
        """Compute DocAI statistics."""
        
        stats = {
            "full_text_len": len(docai_data.get("text", "")),
            "page_count": docai_data.get("page_count", 0),
            "entity_count": docai_data.get("entity_count", 0),
            "clauses_count": 0,
            "key_value_pairs_count": 0,
            "cross_references_count": 0,
            "entity_counts_by_type": {},
            "avg_confidence": 0.0,
            "clause_coverage_ratio": 0.0
        }
        
        try:
            # Count entities by type
            entities = docai_data.get("entities", [])
            for entity in entities:
                entity_type = entity.get("type", "unknown")
                stats["entity_counts_by_type"][entity_type] = stats["entity_counts_by_type"].get(entity_type, 0) + 1
            
            # Count clauses
            clauses = docai_data.get("clauses", [])
            stats["clauses_count"] = len(clauses)
            
            # Calculate clause coverage ratio
            if stats["full_text_len"] > 0 and clauses:
                total_clause_length = sum(
                    clause.get("end_offset", 0) - clause.get("start_offset", 0)
                    for clause in clauses
                    if clause.get("start_offset") is not None and clause.get("end_offset") is not None
                )
                stats["clause_coverage_ratio"] = total_clause_length / stats["full_text_len"]
            
            # Count key-value pairs
            kv_pairs = docai_data.get("key_value_pairs", [])
            stats["key_value_pairs_count"] = len(kv_pairs)
            
            # Count cross-references
            cross_refs = docai_data.get("cross_references", [])
            stats["cross_references_count"] = len(cross_refs)
            
            # Calculate average confidence
            confidences = []
            
            # Entity confidences
            for entity in entities:
                if "confidence" in entity:
                    confidences.append(entity["confidence"])
            
            # Clause confidences
            for clause in clauses:
                if "confidence" in clause:
                    confidences.append(clause["confidence"])
            
            if confidences:
                stats["avg_confidence"] = sum(confidences) / len(confidences)
            
        except Exception as e:
            logger.error(f"DocAI stats computation error: {e}")
            stats["error"] = str(e)
        
        # Save DocAI summary
        with open(self.artifacts_dir / "docai_summary.json", 'w') as f:
            json.dump(stats, f, indent=2)
        
        return stats
    
    def _extract_fallback_kvs(self, vision_data: Dict[str, Any]) -> Dict[str, Any]:
        """Extract key-value pairs using fallback regex patterns."""
        
        # Get full text
        full_text = ""
        if "full_text" in vision_data:
            full_text = vision_data["full_text"]
        elif "ocr_result" in vision_data:
            full_text = vision_data["ocr_result"].get("full_text", "")
        
        # Define regex patterns for common insurance document fields
        patterns = {
            "policy_no": [
                r"Policy\s+No\.?\s*:?\s*([A-Z0-9\-/]+)",
                r"Policy\s+Number\s*:?\s*([A-Z0-9\-/]+)"
            ],
            "date_of_commencement": [
                r"Date\s+of\s+Commencement\s+of\s+Policy\s*:?\s*([0-9\-/\.]+)",
                r"Commencement\s+Date\s*:?\s*([0-9\-/\.]+)"
            ],
            "sum_assured": [
                r"Sum\s+Assured\s+for\s+Basic\s+Plan\s*:?\s*\(?\s*Rs\.?\s*\)?\s*:?\s*([0-9,]+)",
                r"Sum\s+Assured\s*:?\s*\(?\s*Rs\.?\s*\)?\s*:?\s*([0-9,]+)"
            ],
            "dob": [
                r"Date\s+of\s+Birth\s*:?\s*([0-9\-/\.]+)",
                r"DOB\s*:?\s*([0-9\-/\.]+)"
            ],
            "nominee": [
                r"Nominee\s+under\s+section\s+39.*?:?\s*([A-Za-z\s]+)",
                r"Nominee\s*:?\s*([A-Za-z\s]+)"
            ]
        }
        
        extracted = {}
        
        for field, field_patterns in patterns.items():
            extracted[field] = []
            
            for pattern in field_patterns:
                matches = re.findall(pattern, full_text, re.IGNORECASE)
                for match in matches:
                    if match.strip():
                        extracted[field].append({
                            "value": match.strip(),
                            "pattern": pattern,
                            "confidence": "regex_fallback"
                        })
        
        # Save fallback results
        with open(self.artifacts_dir / "vision_fallback_kv.json", 'w') as f:
            json.dump(extracted, f, indent=2)
        
        return extracted
    
    def _save_check_results(self, text_comparison, offset_validation, vision_stats, docai_stats, fallback_kv):
        """Save all check results to separate files."""
        
        # Already saved in individual methods, but ensure they exist
        files_to_check = [
            ("text_diff.txt", "Text comparison saved"),
            ("mismatch_report.json", "Offset validation saved"),
            ("vision_summary.json", "Vision stats saved"),
            ("docai_summary.json", "DocAI stats saved"),
            ("vision_fallback_kv.json", "Fallback KV extraction saved")
        ]
        
        for filename, message in files_to_check:
            file_path = self.artifacts_dir / filename
            if file_path.exists():
                logger.info(f"   âœ… {message}")
            else:
                logger.warning(f"   âŒ Missing: {filename}")
    
    def _compile_diagnostics(self, text_comparison, offset_validation, vision_stats, docai_stats) -> Dict[str, Any]:
        """Compile comprehensive diagnostics with prioritized fixes."""
        
        diagnostics = {
            "timestamp": datetime.now().isoformat(),
            "overall_status": "unknown",
            "failed_checks": [],
            "warnings": [],
            "prioritized_fixes": [],
            "summary": {}
        }
        
        # Check 1: Text matching
        if not text_comparison["exact_match"]:
            severity = "high" if text_comparison["similarity_score"] < 0.8 else "medium"
            diagnostics["failed_checks"].append({
                "check": "text_matching",
                "severity": severity,
                "message": f"Vision and DocAI text mismatch (similarity: {text_comparison['similarity_score']:.2f})"
            })
        
        # Check 2: Offset validation
        if not offset_validation["all_valid"]:
            diagnostics["failed_checks"].append({
                "check": "offset_validation",
                "severity": "high",
                "message": f"{offset_validation['invalid_offsets']} invalid offsets found"
            })
        
        # Check 3: Entity extraction
        if docai_stats["entity_count"] == 0:
            diagnostics["failed_checks"].append({
                "check": "entity_extraction",
                "severity": "high",
                "message": "No entities extracted by DocAI"
            })
        
        # Check 4: Clause segmentation
        if docai_stats["clauses_count"] == 0:
            diagnostics["failed_checks"].append({
                "check": "clause_segmentation",
                "severity": "medium",
                "message": "No clauses extracted by DocAI"
            })
        
        # Generate prioritized fixes
        high_severity = [c for c in diagnostics["failed_checks"] if c["severity"] == "high"]
        medium_severity = [c for c in diagnostics["failed_checks"] if c["severity"] == "medium"]
        
        fixes = []
        
        if high_severity:
            if any("entity" in c["check"] for c in high_severity):
                fixes.append({
                    "priority": 1,
                    "type": "config",
                    "description": "Configure DocAI processor for entity extraction",
                    "action": "Verify DOCAI_PROCESSOR_ID supports entity extraction or switch to appropriate processor"
                })
            
            if any("offset" in c["check"] for c in high_severity):
                fixes.append({
                    "priority": 2,
                    "type": "code",
                    "description": "Fix offset calculation in DocAI parser",
                    "action": "Update services/doc_ai/parser.py to correctly map DocAI response offsets to full text"
                })
            
            if any("text" in c["check"] for c in high_severity):
                fixes.append({
                    "priority": 3,
                    "type": "code",
                    "description": "Normalize text processing between Vision and DocAI",
                    "action": "Ensure consistent text normalization in both pipelines"
                })
        
        # Add medium priority fixes
        if not fixes and medium_severity:
            fixes.append({
                "priority": 1,
                "type": "config",
                "description": "Enable clause segmentation in DocAI processor",
                "action": "Configure processor to extract document structure and clauses"
            })
        
        # Default fixes if no specific issues found
        if not fixes:
            fixes = [
                {"priority": 1, "type": "config", "description": "Optimize DocAI processor configuration", "action": "Review processor settings for better extraction"},
                {"priority": 2, "type": "code", "description": "Enhance error handling", "action": "Add better error handling and logging"},
                {"priority": 3, "type": "monitoring", "description": "Add performance monitoring", "action": "Implement latency and accuracy tracking"}
            ]
        
        diagnostics["prioritized_fixes"] = fixes[:3]  # Top 3 fixes
        
        # Overall status
        if not diagnostics["failed_checks"]:
            diagnostics["overall_status"] = "healthy"
        elif any(c["severity"] == "high" for c in diagnostics["failed_checks"]):
            diagnostics["overall_status"] = "critical"
        else:
            diagnostics["overall_status"] = "degraded"
        
        # Summary
        diagnostics["summary"] = {
            "total_checks": 4,
            "failed_checks": len(diagnostics["failed_checks"]),
            "warnings": len(diagnostics["warnings"]),
            "overall_status": diagnostics["overall_status"]
        }
        
        return diagnostics
    
    def generate_e2e_report(self) -> str:
        """Generate end-to-end report with run log and latencies."""
        
        report_lines = [
            "Vision â†’ DocAI Pipeline Diagnostics Report",
            "=" * 50,
            f"Generated: {datetime.now().isoformat()}",
            f"Test PDF: {self.test_pdf_path}",
            "",
            "EXECUTION LOG:",
            "-" * 20
        ]
        
        # Add timing information
        timing = self.results.get("timing", {})
        if timing:
            report_lines.extend([
                "",
                "LATENCIES:",
                "-" * 10
            ])
            for phase, duration in timing.items():
                report_lines.append(f"{phase.replace('_', ' ').title()}: {duration:.3f}s")
        
        # Add component results
        vision_result = self.results.get("vision_ocr", {})
        docai_result = self.results.get("docai_processing", {})
        comparison_result = self.results.get("comparison", {})
        
        report_lines.extend([
            "",
            "COMPONENT RESULTS:",
            "-" * 18,
            f"Vision OCR: {'âœ… SUCCESS' if vision_result.get('success') else 'âŒ FAILED'}",
            f"DocAI Processing: {'âœ… SUCCESS' if docai_result.get('success') else 'âŒ FAILED'}",
            f"Text Matching: {'âœ… EXACT' if comparison_result.get('text_match') else 'âŒ MISMATCH'}",
            f"Offset Validation: {'âœ… VALID' if comparison_result.get('offsets_valid') else 'âŒ INVALID'}"
        ])
        
        # Add errors
        if self.results.get("errors"):
            report_lines.extend([
                "",
                "ERRORS:",
                "-" * 7
            ])
            for error in self.results["errors"]:
                report_lines.append(f"â€¢ {error}")
        
        # Exit code determination
        exit_code = 0
        if self.results.get("errors") or not (vision_result.get("success") and docai_result.get("success")):
            exit_code = 1
        
        report_lines.extend([
            "",
            f"EXIT CODE: {exit_code}",
            ""
        ])
        
        report_content = "\n".join(report_lines)
        
        # Save report
        with open(self.artifacts_dir / "e2e_report.txt", 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        return report_content
    
    def run_full_diagnostics(self) -> Dict[str, Any]:
        """
        Run complete diagnostics pipeline.
        
        Returns:
            Complete diagnostic results
        """
        logger.info("ðŸ” Starting Vision â†’ DocAI Pipeline Diagnostics")
        logger.info("=" * 80)
        
        overall_start = time.time()
        
        try:
            # Step 1: Vision OCR processing
            vision_result = self.run_vision_ocr_processing()
            
            # Step 2: DocAI processing
            docai_result = self.run_docai_processing()
            
            # Step 3: Automated checks
            comparison_result = self.run_automated_checks()
            
            # Step 4: Generate final report
            e2e_report = self.generate_e2e_report()
            
            total_time = time.time() - overall_start
            self.results["timing"]["total_diagnostics"] = total_time
            
            logger.info("=" * 80)
            logger.info("ðŸŽ¯ DIAGNOSTICS COMPLETED")
            logger.info(f"Total time: {total_time:.2f}s")
            logger.info(f"Artifacts saved to: {self.artifacts_dir}")
            logger.info("=" * 80)
            
            return self.results
            
        except Exception as e:
            logger.error(f"âŒ Full diagnostics failed: {e}")
            traceback.print_exc()
            self.results["errors"].append(f"Full diagnostics error: {str(e)}")
            
            # Generate error report
            self.generate_e2e_report()
            
            return self.results
    
    def answer_diagnostic_questions(self) -> Dict[str, Any]:
        """
        Answer the 6 required diagnostic questions.
        
        Returns:
            Structured answers to diagnostic questions
        """
        logger.info("=" * 60)
        logger.info("DIAGNOSTIC QUESTIONS ANALYSIS")
        logger.info("=" * 60)
        
        answers = {}
        
        try:
            # Load data files
            docai_raw_path = self.artifacts_dir / "docai_raw.json"
            vision_raw_path = self.artifacts_dir / "vision_raw.json"
            
            docai_data = {}
            vision_data = {}
            
            if docai_raw_path.exists():
                with open(docai_raw_path) as f:
                    docai_data = json.load(f)
            
            if vision_raw_path.exists():
                with open(vision_raw_path) as f:
                    vision_data = json.load(f)
            
            # Question 1: DocAI content analysis
            logger.info("1ï¸âƒ£ Analyzing DocAI raw content...")
            
            entities = docai_data.get("entities", [])
            clauses = docai_data.get("clauses", [])
            kv_pairs = docai_data.get("key_value_pairs", [])
            cross_refs = docai_data.get("cross_references", [])
            
            answers["q1_docai_content"] = {
                "has_clause_segmentation": len(clauses) > 0,
                "has_named_entities": len(entities) > 0,
                "has_key_value_pairs": len(kv_pairs) > 0,
                "has_cross_references": len(cross_refs) > 0,
                "counts": {
                    "clauses": len(clauses),
                    "entities": len(entities),
                    "key_value_pairs": len(kv_pairs),
                    "cross_references": len(cross_refs)
                }
            }
            
            # Question 2: Text matching
            logger.info("2ï¸âƒ£ Checking text matching...")
            
            vision_text = vision_data.get("full_text", "")
            if not vision_text and "ocr_result" in vision_data:
                vision_text = vision_data["ocr_result"].get("full_text", "")
            
            docai_text = docai_data.get("text", "")
            
            # Normalize and compare
            vision_clean = " ".join(vision_text.split())
            docai_clean = " ".join(docai_text.split())
            text_match = vision_clean == docai_clean
            
            # Get first 200 char diff if not matching
            first_200_diff = None
            if not text_match:
                vision_200 = vision_text[:200]
                docai_200 = docai_text[:200]
                first_200_diff = {
                    "vision_first_200": vision_200,
                    "docai_first_200": docai_200,
                    "difference": f"Vision starts: {repr(vision_200[:50])}...\nDocAI starts: {repr(docai_200[:50])}..."
                }
            
            answers["q2_text_matching"] = {
                "texts_match": text_match,
                "vision_length": len(vision_text),
                "docai_length": len(docai_text),
                "first_200_diff": first_200_diff
            }
            
            # Question 3: Offset validation
            logger.info("3ï¸âƒ£ Validating offsets...")
            
            mismatch_report_path = self.artifacts_dir / "mismatch_report.json"
            offset_validation = {"all_valid": True, "failures": []}
            
            if mismatch_report_path.exists():
                with open(mismatch_report_path) as f:
                    offset_validation = json.load(f)
            
            answers["q3_offset_validation"] = {
                "offsets_valid": offset_validation["all_valid"],
                "total_entities": offset_validation.get("total_entities", 0),
                "valid_offsets": offset_validation.get("valid_offsets", 0),
                "invalid_offsets": offset_validation.get("invalid_offsets", 0),
                "failure_list": offset_validation.get("failures", [])[:5]  # First 5 failures
            }
            
            # Question 4: Mandatory KV extraction
            logger.info("4ï¸âƒ£ Checking mandatory KV extraction...")
            
            fallback_kv_path = self.artifacts_dir / "vision_fallback_kv.json"
            fallback_kv = {}
            
            if fallback_kv_path.exists():
                with open(fallback_kv_path) as f:
                    fallback_kv = json.load(f)
            
            mandatory_kvs = ["policy_no", "date_of_commencement", "sum_assured", "dob", "nominee"]
            kv_extraction_status = {}
            
            for kv_field in mandatory_kvs:
                # Check DocAI extraction
                docai_found = any(
                    kv_field.lower() in kv.get("key", "").lower()
                    for kv in docai_data.get("key_value_pairs", [])
                )
                
                # Check fallback extraction
                fallback_found = len(fallback_kv.get(kv_field, [])) > 0
                
                kv_extraction_status[kv_field] = {
                    "docai_extracted": docai_found,
                    "fallback_extracted": fallback_found,
                    "fallback_values": fallback_kv.get(kv_field, [])[:2]  # First 2 values
                }
            
            answers["q4_mandatory_kvs"] = kv_extraction_status
            
            # Question 5: Top 3 prioritized fixes
            logger.info("5ï¸âƒ£ Generating prioritized fixes...")
            
            diagnostics_path = self.artifacts_dir / "diagnostics.json"
            prioritized_fixes = []
            
            if diagnostics_path.exists():
                with open(diagnostics_path) as f:
                    diag_data = json.load(f)
                    prioritized_fixes = diag_data.get("prioritized_fixes", [])
            
            answers["q5_prioritized_fixes"] = prioritized_fixes[:3]
            
            # Question 6: Commands and errors
            logger.info("6ï¸âƒ£ Documenting commands and errors...")
            
            answers["q6_execution_info"] = {
                "commands_run": [
                    "python -m venv .venv",
                    "pip install -r requirements.txt",
                    "python scripts/test_vision_to_docai.py"
                ],
                "errors_encountered": self.results.get("errors", []),
                "stack_traces": [],  # Would be populated if we caught specific traces
                "environment_status": {
                    "google_project_id": bool(self.config["google_project_id"]),
                    "credentials_path": bool(self.config["google_credentials"]),
                    "docai_processor_id": bool(self.config["docai_processor_id"])
                }
            }
            
            # Save complete answers
            with open(self.artifacts_dir / "diagnostic_answers.json", 'w') as f:
                json.dump(answers, f, indent=2)
            
            logger.info("âœ… Diagnostic questions answered")
            return answers
            
        except Exception as e:
            logger.error(f"âŒ Failed to answer diagnostic questions: {e}")
            return {"error": str(e)}


def main():
    """Main function to run complete diagnostics."""
    
    diagnostics = VisionDocAIDiagnostics()
    
    try:
        # Run full diagnostics
        results = diagnostics.run_full_diagnostics()
        
        # Answer diagnostic questions
        answers = diagnostics.answer_diagnostic_questions()
        
        # Print summary
        print("\n" + "=" * 80)
        print("ðŸŽ¯ DIAGNOSTIC SUMMARY")
        print("=" * 80)
        
        # Overall status
        vision_success = results.get("vision_ocr", {}).get("success", False)
        docai_success = results.get("docai_processing", {}).get("success", False)
        
        if vision_success and docai_success:
            print("âœ… PIPELINE STATUS: HEALTHY")
            exit_code = 0
        elif vision_success or docai_success:
            print("âš ï¸ PIPELINE STATUS: PARTIAL")
            exit_code = 1
        else:
            print("âŒ PIPELINE STATUS: FAILED")
            exit_code = 2
        
        # Print key metrics
        print(f"\nProcessing Times:")
        for phase, duration in results.get("timing", {}).items():
            print(f"  {phase.replace('_', ' ').title()}: {duration:.3f}s")
        
        print(f"\nArtifacts Generated:")
        artifacts = list(diagnostics.artifacts_dir.glob("*.json")) + list(diagnostics.artifacts_dir.glob("*.txt"))
        for artifact in sorted(artifacts):
            print(f"  âœ… {artifact.name}")
        
        print(f"\nErrors: {len(results.get('errors', []))}")
        for error in results.get("errors", [])[:3]:
            print(f"  â€¢ {error}")
        
        print(f"\nðŸ“ All artifacts saved to: {diagnostics.artifacts_dir}")
        print(f"ðŸ” Review diagnostics.json for detailed analysis")
        
        return exit_code
        
    except Exception as e:
        logger.error(f"âŒ Diagnostics failed: {e}")
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)